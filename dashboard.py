# streamlit run dashboard.py
import streamlit as st
import pandas as pd
import json
import plotly.graph_objs as go
# from mizani.utils import multitype_sort
from plotnine import ggplot, aes, geom_count, theme_light, geom_boxplot, coord_flip

df = pd.read_csv('./Data/de_cleaned.csv')
df['START_DATE'] = pd.to_datetime(df['START_DATE']).dt.date
df = df.rename({'ILL_YR': 'ILL_NMU_YR', 'ILL_MNTH': 'ILL_NMU_MNTH', 'ILL_WK': 'ILL_NMU_WK'}, axis = 1)
df_model = pd.read_csv('./Data/model_comparison.csv')
table_lr = pd.read_csv('./Data/table_lr.csv')
table_gb = pd.read_csv('./Data/table_gb.csv')
table_rf = pd.read_csv('./Data/table_rf.csv')
df_causal = pd.read_csv('./Data/df_causal.csv')
session = st.sidebar.selectbox("Section", ["Introduction", "EDA", "Predictive Model", "Causal Inference"])
st.title('German NMURx Dashboard')

# Used function
def plot_state_map_new(df, width=700, height=700):
    """
    df contains predictors and response variables,
    It must contain 'DEM_LOCATION' column and a response var column named 'y'.
    """

    # read state polygon information as a json file
    f = open("./Data/2_hoch.geo.json")
    germ_states = json.load(f)
    f.close()

    # rename the id in the json file and create the state-location map relationship
    states = []
    locations = []
    for i in range(len(germ_states["features"])):
        germ_states["features"][i]["id"] = germ_states["features"][i]["properties"]["name"]
        states.append(germ_states["features"][i]["properties"]["name"])
        locations.append(i + 1)
    locations[13] = 13
    locations[12] = 14

    state_loc_map = pd.DataFrame({"location": locations, "state": states})

    # state-location map

    # 1. calculate proportion of y == 1 in each location number
    # 2. map the state name to the location num
    y_prop = df[["DEM_LOCATION", "y"]].groupby(by="DEM_LOCATION").mean("y").reset_index()
    y_prop = y_prop.merge(state_loc_map, left_on="DEM_LOCATION", right_on="location", how="left")[
        ["DEM_LOCATION", "y", "state"]]

    # plot the figure using plotly.graphic_objects
    fig = go.Figure(data=go.Choropleth(
        locations=y_prop['state'],
        z=y_prop['y'].astype(float),
        locationmode='geojson-id',
        colorscale="Viridis",
        autocolorscale=False,
        marker_line_color='white',  # line markers between states
        colorbar_title="Case Proportion"
    ))

    fig.update_traces(geojson=germ_states, selector=dict(type='choropleth'), reversescale=True)
    fig.update_geos(scope="europe", fitbounds="locations", visible=False)
    fig.update_layout(width=width, height=height)
    return fig

if session == "Introduction":

    st.sidebar.subheader("Welcome to our dashboard!")

    st.image('./Img/germany.png', width = 600)
    st.subheader("Introduction")
    st.write("""
    This dashboard is created in a 47-hour DataFest competition. The major objective is to 
    analyze the non-medical use of prescription drugs in Germany. The dashboard contains three sections: 
    
    - EDA
    - Predictive Model
    - Causal Inference.
    
    In the EDA section, we tried plots between pairs of variables to find intuitions. 
    After trial, we decided to analyze the relationship between drug use in certain 
    time interval (within 1 year/month/week or whole lifetime) and personal 
    information & lifestyle. At the same time, we also created a map plot to show 
    drug use proportion in different regions in Germany using zip code information.
    
    In the Predictive Model section, we tried 14 kinds of classifier to train the 
    dataset and compare their accuracy, F1 score and time efficiency. Specifically, 
    we chose three models (Logistic Regression & Gradient Boosting Classifier & Random 
    Forest Classifier) , tuned their parameters and compared the importance of predictors 
    generated by them.
    
    In the Causal Inference, we firstly analyzed the drug use distribution in pregnant 
    women samples. Due to the covariate imbalance in this dataset, we used the weighted causal 
    inference model (Inverse Prebability Weighting & Overlap Weighting) to test if the 
    difference between groups are significant under balance. Similarly, we also conducted 
    inference for different genders, income levels, survey languages etc.""")

if session == "EDA":

    st.sidebar.subheader("Exploratory Data Analysis")
    parts_eda = st.sidebar.radio("Two Parts:", ["General EDA", "Location Analysis"])
    # sidebars
    date_range = st.sidebar.slider('Range of survey date:', min(df['START_DATE']), max(df['START_DATE']), (pd.to_datetime('2017-12-17').date(), pd.to_datetime('2018-01-11').date()))
    y_time = st.sidebar.selectbox("Period of outcome variable:", ['Lifetime', 'Last year', 'Last 30 days', 'Last 7 days'], index = 0)
    y_type = st.sidebar.multiselect('Drug included to create the outcome variable', ['Opioid', 'Benzodiazepine', 
        'Stimulant', 'GABA-analogue', 'Illicit'], default = ['Opioid', 'Benzodiazepine', 'Stimulant', 'GABA-analogue', 'Illicit'])
    y_explain = st.sidebar.checkbox('The explanation of outcome variable')
    if y_explain:
        st.sidebar.write("""The creation of outcome variable is based on *Drugs 
            included to create the outcome variable*, and the calculation period is determined 
            by *Period of outcome variable*. If any drug within 
            the selection range is used, the outcome variable is set to be $1$. Otherwise, the 
            outcome variable will be $0$.""")
    match1 = {'Lifetime': '_USE', 
        'Last year': '_NMU_YR',
        'Last 30 days': '_NMU_MNTH', 
        'Last 7 days': '_NMU_WK'}
    match2 = {'Opioid': 'OP', 
        'Benzodiazepine': 'BENZ', 
        'Stimulant': 'STIM', 
        'GABA-analogue': 'GABA', 
        'Illicit': 'ILL'}
    df = df[(df['START_DATE'] >= date_range[0]) & (df['START_DATE'] <= date_range[1])]
    df_outcome = df[[match2[i] + match1[y_time] for i in y_type]]
    df['y'] = df_outcome.sum(axis = 1) > 0
    df = df.drop(df.loc[:, 'OP_USE':'ILL_NMU_WK'].columns, axis = 1).drop('START_DATE', axis = 1)
    
    if parts_eda == 'General EDA':
        # sidebars
        df['DEM_GENDER'] = df['DEM_PREG'].replace({'0.0': 'female', '1.0': 'female'})
        preds = df.drop(['DEM_LOCATION', 'DEM_PREG', 'DEM_PREGMNTH', 'y'], axis = 1).columns
        pred_type = st.sidebar.selectbox("Predictor type:", ['Demographics', 'Past history'], index = 0)
        if pred_type == 'Demographics':
            predictor = st.sidebar.selectbox('Predictor:', preds[preds.str.contains('DEM')], index = 0)
        else:
            predictor = st.sidebar.selectbox('Predictor:', preds[~preds.str.contains('DEM')], index = 0)

        # plot
        st.write("""In this section, we will visualize the relationship between each predictor and 
        the outcome variable. The creation process of the outcome variable is customized and can be
        revised by changing the sidebar options. For numeric predictors, the visualization will be 
        a box plot. For categorical predictors, 2-dimensional frequency graphs can be applied to reveal 
        the relationship, where the size of each node represents the number of samples falling into the 
        corresponding categories.""")
        st.subheader("The relationship between {} and outcome variable".format(predictor))
        if predictor != 'DEM_AGE':
            df_eda = df[['y', predictor]]
            df_eda[predictor] = df_eda[predictor].astype('object')
            p = (ggplot(data = df_eda) + 
            geom_count(mapping = aes(x = predictor, y = 'y')) +
            theme_light(base_size = 15))
            st.pyplot(ggplot.draw(p))
        if predictor == 'DEM_AGE':
            p = (ggplot(data = df[['y', 'DEM_AGE']]) + 
            geom_boxplot(mapping = aes(y = 'DEM_AGE', x = 'y'), notch = True) +
            coord_flip() + 
            theme_light(base_size = 15))
            st.pyplot(ggplot.draw(p))
        st.write("""**Note**: Due to the requirement of data privacy, the detailed definition of each 
        category cannot be given, but the basic analytical logic can be obtained.""")

    if parts_eda == 'Location Analysis':
        st.write("""This section analyzes the geographic distribution of the proportion of people in Germany 
        who use prescription drugs in a non-medical manner in the survey population. The definition of non-medical 
        use of prescription drugs is customized and can be revised by changing the sidebar options. The analysis 
        is based on the unit of regions in Germany. The shade of the map indicates the magnitude of proportion.""")
        st.subheader("Distribution of Non-Medical Use of Prescription Drugs in German")
        fig_map = plot_state_map_new(df[['DEM_LOCATION', 'y']])
        st.plotly_chart(fig_map)

if session == "Predictive Model":

    st.sidebar.subheader("Predictive Model")
    parts_model = st.sidebar.radio("Two Parts:", ["Model Comparison", "Model Result"])
    y_explain = st.sidebar.checkbox('The explanation of outcome variable')
    if y_explain:
        st.sidebar.write("""To ensure data balance, the outcome variable used to construct the predictive model 
            is the combination of lifetime uses of opioid, benzodiazepine, stimulant, GABA-analogue and illicit. 
            If any drug within the selection range is used, the outcome variable 
            is set to be $1$. Otherwise, the outcome variable will be $0$.""")

    if parts_model == 'Model Comparison':
        # sidebar
        order = st.sidebar.selectbox("Sort criteria:", ['Accuracy', 'F1 Score', 'Training Time'], index = 0)
        st.sidebar.write('The default order is descending.')
        ascend = st.sidebar.checkbox('Ascending')
        num_var = st.sidebar.number_input("Number of models to show (up to 14):", 1, 14, 14)

        match3 = {'Accuracy': 'Accuracy', 'F1 Score': 'F1', 'Training Time': 'TT (Sec)'}
        st.subheader("Model Comparison Result")
        st.write(df_model.sort_values(match3[order], ascending = ascend).reset_index(drop = True).iloc[:num_var, :])
        st.write("""In the table above, we see four types of models, and their sorting criterion can be changed using the sidebars.  
        **1. Logistic Regression with/without Regularization**  
        Logistic Regression is generalized linear model. Linear model provides the BEST LINEAR UNBIASED 
        ESTIMATE. Regularization improves the ability of generalization, so it has the potential to 
        improve cross validation metrics. In this dataset, linear model performs the best if default 
        hyperparameters are applied in all models.  
        **2. Boosting Models**  
        From statistical learning theory, we know that the MSE can be decomposed into two parts: 
        variance and bias. Model training is usually a tradeoff between this two errors. Boosting 
        model focuses on decreasing the bias through gradient or other mathematical tools. Therefore, 
        this model is highly likely to give accurate predictions on drug use.  
        **3. Tree-based Bagging Models**  
        In contrast with boosting, bagging aims at decreasing the variance by taking average of 
        independent sub-models. For example, in Ramdom Forest Classifier, we trained a large number 
        of decision trees and took their average. This Dataset contains a large proportion of categorical variables, 
        which is an appropriate scenario for decision tree uses.  
        **4. Other Models**  
        SVM, Naive Bayes and other classifiers did not show good performance, probably because most of them 
        require strong assumptions on the dataset. SVM assumes a certain shape of decision boundary and 
        Naive Bayes requires that the predictors are independent with each other. These requirements are hard to 
        be fulfilled in real scenerio.  
        We choose one model in each of the types above (except other models) and conducted potential parameters 
        tuning. Please select **Model Result** in the left sidebar to see details.""")

    if parts_model == 'Model Result':
        model = st.sidebar.selectbox("Select a Model:", ['Logistic Regression', 'Gradient Boosting Classifier', 'Random Forest Classifier'], index = 0)

        if model == 'Logistic Regression':
            st.subheader('Top 10 Important Features')
            st.write(table_lr)
            st.write('**10-fold Cross Validation Accuracy: $70.21\%$**')
            st.write("""The 10-fold cross validation accuracy for the logistic regression is the same as 
            the result in the **Model Comparison** section. This is due to the fact that no parameters in 
            the logistic regression model need to be tuned.  
            Among the top 10 important features, *PAIN_ACUTE*, *PAIN_CHRONIC*, *RXDRUGSAFE_PAIN*, 
            *TOB_USE*, *MENT_ADHD* and *MENT_DEP* are the common important features in the three models. 
            It might indicate that past medical history and life style can have a huge impact in a person's 
            probility to use prescription drugs in a non-medical manner. And this result can be further researched in the 
            future from the medical perspective.""")

        if model == 'Gradient Boosting Classifier':
            st.subheader('Top 10 Important Features')
            st.write(table_gb)
            st.write('**10-fold Cross Validation Accuracy: $70.41\%$**')
            para_tuning = st.sidebar.checkbox('Show hyperparameter tuning results')
            if para_tuning:
                st.subheader('GBM Hyperparameter Tuning Result')
                st.image('./Img/tuning_gb.png', width = 700)
            st.write("""The 10-fold cross validation accuracy for the gradient boosting machine is different from  
            the result in the **Model Comparison** section. This is due to the hyperparameter tuning process that can 
            slightly improve the model performance.  
            Among the top 10 important features, *PAIN_ACUTE*, *PAIN_CHRONIC*, *RXDRUGSAFE_PAIN*, 
            *TOB_USE*, *MENT_ADHD* and *MENT_DEP* are the common important features in the three models. 
            It might indicate that past medical history and life style can have a huge impact in a person's 
            probility to use prescription drugs in a non-medical manner. And this result can be further researched in the 
            future from the medical perspective.""")
        if model == 'Random Forest Classifier':
            st.subheader('Top 10 Important Features')
            st.write(table_rf)
            st.write('**10-fold Cross Validation Accuracy: $69.50\%$**')
            para_tuning = st.sidebar.checkbox('Show hyperparameter tuning results')
            if para_tuning:
                st.subheader('Random Forest Hyperparameter Tuning Result')
                st.image('./Img/tuning_rf.png', width = 700)
            st.write("""The 10-fold cross validation accuracy for the random forest algorithm is different from  
            the result in the **Model Comparison** section. This is due to the hyperparameter tuning process that can 
            slightly improve the model performance.  
            Among the top 10 important features, *PAIN_ACUTE*, *PAIN_CHRONIC*, *RXDRUGSAFE_PAIN*, 
            *TOB_USE*, *MENT_ADHD* and *MENT_DEP* are the common important features in the three models. 
            It might indicate that past medical history and life style can have a huge impact in a person's 
            probility to use prescription drugs in a non-medical manner. And this result can be further researched in the 
            future from the medical perspective.""")

if session == "Causal Inference":
    st.sidebar.subheader("Causal Inference")
    st.sidebar.write("""The objective of this section is to apply weighting technology in the causal inference field to 
    solve the problems that we are particularly interested in.""")

    st.subheader("Overview about the Influence of Pregnancy")
    st.image('./Img/combined_plot.png', width = 700)
    st.write("""For pregnant women, we compare the proportion among different pregnant months (1-9). 
    Generally, as month increases, the proportion shows decreasing trend, shown in *Figure 1*. 
    We also observe relatively unstable trend for 7 months and 8 months. From *Figure 2*, we 
    know that the sample size of these "month" is small compared with others, which might lead to 
    huge variance when generalized into population. We believe that if we increase the sample size, 
    these 3 proportions will fit with the general trend.""")

    st.subheader("Causal Inference Analysis")
    st.write(df_causal)
    st.write("""In this part, we conduct causal inference analysis between some predictors and y 
    ("ILL_USE"). Theoretically, t-test is the best option in this scenario. However, the sample 
    size is large enough so that the t-distrition with a large degree of freedom is almost a normal 
    distribution, so we compute the z-score (ASD) of the mean difference between two groups and 
    compared with $97.5\%$ quantile $1.96$. To ensure the balance of observed covariates, we also compute 
    weighted z-score (ASD) with **Inverse Probability Weighting(IPW)** and **Overlap Weighting(OW)**.""")
    st.image('./Img/scatter_plot_withlabel.png', width = 700)
    st.write("""We find three kinds of patterns:  
    **1. Gender**: All three z-scores are smaller than threshold, there is no significant difference 
    betwen male and female.  
    **2. Pregnant or not, Pregnant Month**: The original ASD is smaller than threshold while weighted 
    ASD is larger than threshold. This is probably because of the imbalance for these two variables. If 
    we use weighted ASD to reduce or even eliminate the imbalance, we can conclude a significant 
    difference between pregnant and unpregnant women. In the previous part, we get the decreasing 
    trend for increasing pregnant month. Here we confirm that the difference is significant under 
    balanced casual inference.  
    **3. Smoke, Income, Language**: All three z=scores are bigger than threshold, the difference is 
    significant between corresponding groups.""")

    st.write("""**Note:**  
    The original ASD is computed by:  
    $$ASD=\\frac{|\\frac{\sum_{i=1}^{N}X_iZ_i}{\sum_{i=1}^{N}Z_i}-\\frac{\sum_{i=1}^{N}X_i(1-Z_i)}{\sum_{i=1}^{N}(1-Z_i)}|}{\sqrt{s_1^2/N_1 + s_2^2/N_0}}$$  
    Here $X$ is the target variable we want to compare, $Z$ is the group variable. $s_1^2$ and $s_0^2$ are normal sample variance for two groups. $N_1$ and $N_0$ are sample size.  
    The weighted ASD is computed by:  
    $$ASD = \\frac{|\\frac{\sum_{i=1}^{N}X_iZ_iw_1(X_i)}{\sum_{i=1}^{N}Z_iw_1(X_i)}-\\frac{\sum_{i=1}^{N}X_i(1-Z_i)w_0(X_i)}{\sum_{i=1}^{N}(1-Z_i)w_0(X_i)}|}{\sqrt{s_1^2/N_1 + s_2^2/N_0}}$$  
    For IPW, $w_1(X) = \\frac{1}{e(X)}$, $w_0 = \\frac{1}{1-e(X)}$.  
    For OW, $w_1(X) = 1 - e(X)$, $w_0 = e(X)$.  
    Here $e(X)$ is the **propensity score**, defined as $e(X) = Pr(Z = 1|X) = E(Z|X)$, computed with logistic regression with $X$ and $Z$.  
    **Theorem$^{[1]}$:** When the propensity scores are estimated by maximum likelihood under a logistic regression model, the overlap weights lead to exact balance in the means of any included covariate between treatment and control groups.""")

    st.subheader("Reference")
    st.write("""[1] Li, Fan, Kari Lock Morgan, and Alan M. Zaslavsky. “Balancing covariates via propensity score weighting.” Journal of the American Statistical Association 113.521 (2018): 390-400.""")
